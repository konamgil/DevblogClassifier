{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Code Generator\n",
    "- `%%writefile ./name.py`\n",
    " - jupyter에서 작업한 내용을 .py 파일로 쉽게 내보내기 위해 magic command 이용\n",
    " - 첫 줄의 %%writefile을 주석처리 하지 않으면 해당 셀 내용을 `name.py` 파일로 export\n",
    " - 첫 줄의 %%writefile을 주석처리하면 jupyter에서 실행됨\n",
    " - 코딩을 .py파일이 아닌 jupyter에서 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./main.py\n",
    "from document import Document\n",
    "from analysis import Analysis\n",
    "from word_vector import WordVector\n",
    "from classifier import Classifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc = Document()\n",
    "wv = WordVector()\n",
    "cf = Classifier()\n",
    "wv_model = wv.getWikiModel()\n",
    "model = cf.loadModel()\n",
    "\n",
    "def train(epochs=10,\n",
    "          batch_size=100,\n",
    "          validation_split=0.1,\n",
    "          verbose=0,\n",
    "          checkpoint_path='./model/checkpoints'):\n",
    "    \"\"\"\n",
    "    classifier 학습\n",
    "    \n",
    "    - input\n",
    "    : epochs / int / 학습 횟수\n",
    "    : batch_size / int / 배치 사이즈\n",
    "    : validation_split / float / validation data ratio\n",
    "    : checkpoint_path / str / 학습 중간 결과물 저장 경로\n",
    "    \n",
    "    - export\n",
    "    : ./model/classifier.json (graph)\n",
    "    : ./model/classifier.h5 (weights)\n",
    "    \"\"\"\n",
    "    \n",
    "    # load data\n",
    "    data = getTrainData()\n",
    "    \n",
    "    # train\n",
    "    model = cf.train(data,\n",
    "                 checkpoint_path=checkpoint_path,\n",
    "                 epochs=epochs,\n",
    "                 batch_size=batch_size,\n",
    "                 validation_split=validation_split,\n",
    "                 verbose=verbose)\n",
    "    cf.showHistory()\n",
    "\n",
    "def getTrainData():\n",
    "    \"\"\"\n",
    "    라벨링 된 데이터를 임베딩하여 반환\n",
    "    \n",
    "    - return\n",
    "    : DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # 라벨링 된 데이터만 가져오기\n",
    "    data = doc.getDocs(True) \n",
    "    \n",
    "    # vectorization\n",
    "    data['vector'] = data.text.apply(lambda x: wv.vectorization(wv_model, x))\n",
    "    return data\n",
    "\n",
    "def predict(text, criterion=0.5):\n",
    "    \"\"\"\n",
    "    개발관련 문서여부 반환\n",
    "    \n",
    "    - input\n",
    "    : text / str / 확인하려는 문서 내용 (영어 또는 한글이 포함되어있어야함)\n",
    "    : criterion / float / 개발관련 문서 판단 기준\n",
    "    \n",
    "    - return\n",
    "    : boolean / 개발문서 여부\n",
    "    : float / 1에 가까울수록 개발관련 문서\n",
    "    \"\"\"\n",
    "    data = doc.preprocessing(pd.DataFrame([{\n",
    "        'title': text,\n",
    "        'description': '',\n",
    "        'tags': []\n",
    "    }]))\n",
    "    data = data.text.apply(lambda x: wv.vectorization(wv_model, x)).tolist()\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print('text is not valid')\n",
    "        return\n",
    "    data = np.array(data)\n",
    "    confidence = round(model.predict(data)[0][1], 3)\n",
    "    is_dev_doc = confidence > criterion\n",
    "    return is_dev_doc, confidence\n",
    "\n",
    "def getDataAnalysis():\n",
    "    \"\"\"\n",
    "    가지고있는 전체 데이터 분석\n",
    "    : label 별 수량\n",
    "    : 문장 길이 histogram\n",
    "    : WordCloud \n",
    "    \"\"\"\n",
    "    # 학습 데이터 분석\n",
    "    Analysis(doc.getDocs(labeled_only=False))\n",
    "    \n",
    "def getSimilarWords(text, topn=5):\n",
    "    \"\"\"\n",
    "    단어 임베딩 모델을 이용하여 주어진 단어와 유사도가 높은 단어를 반환\n",
    "    \n",
    "    - input\n",
    "    : text / str / 유사도를 구하려는 단어\n",
    "    : topn / int / 조회하려는 단어의 개수(유사도가 높은 순서로 자름)\n",
    "    \n",
    "    - return\n",
    "    : list / [(string : word, float : similarity)]\n",
    "    \"\"\"\n",
    "    # 유사 단어 조회\n",
    "    return wv.getSimilarWords(wv_model, text, topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./util.py\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def downloadByURL(url, output_path):\n",
    "    \"\"\"\n",
    "    HTTP 파일 다운로드\n",
    "    \n",
    "    - input\n",
    "    : url / str / 다운로드 받으려는 파일의 url\n",
    "    : output_path / str / 파일 저장 경로\n",
    "    \"\"\"\n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "        \n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./analysis.py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class Analysis():\n",
    "     \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        데이터의 수량, 길이, WordCloud 분석\n",
    "        \n",
    "        - input\n",
    "        : data / DataFrame / documents.csv 데이터\n",
    "        \"\"\"\n",
    "        self.countAnalysis(data)\n",
    "        print()\n",
    "        self.textAnalysis(data)\n",
    "        print()\n",
    "        self.showWordCloud(data.text)\n",
    "        \n",
    "    def countAnalysis(self, data):\n",
    "        \"\"\"\n",
    "        데이터 수량 조사\n",
    "        \n",
    "        - input\n",
    "        : data / DataFrame / documents.csv 데이터\n",
    "        \"\"\"\n",
    "        \n",
    "        labeled_data = data.loc[data.label != -1]\n",
    "        total_count = len(data) # 전체 데이터 수\n",
    "        labeled_count = len(labeled_data) # 라벨링 된 데이터 수\n",
    "\n",
    "        print('> 데이터 수량 조사')\n",
    "        print(f'전체 데이터 수: {total_count}개')\n",
    "        print(f'라벨링된 데이터 수: {labeled_count}개')\n",
    "        for label, count in data.label.value_counts().iteritems():\n",
    "            print(f'class {label} : {count}개')\n",
    "    \n",
    "    def textAnalysis(self, data):\n",
    "        \"\"\"\n",
    "        text 길이 분석\n",
    "        \n",
    "        - input\n",
    "        : data / DataFrame / documents.csv 데이터\n",
    "        \"\"\"\n",
    "        text_len = data.text.apply(len)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.hist(text_len, bins=200, alpha=0.5, color= 'r', label='length of text')\n",
    "        plt.legend(fontsize='x-large')\n",
    "        plt.yscale('log', nonposy='clip')\n",
    "        plt.title('Log-Histogram of length of text')\n",
    "        plt.xlabel('Length of text')\n",
    "        plt.ylabel('Number of text')\n",
    "\n",
    "        print('> 문장 길이 분석')\n",
    "        print('문장 길이 최대 값: {}'.format(np.max(text_len)))\n",
    "        print('문장 길이 최소 값: {}'.format(np.min(text_len)))\n",
    "        print('문장 길이 평균 값: {:.2f}'.format(np.mean(text_len)))\n",
    "        print('문장 길이 표준편차: {:.2f}'.format(np.std(text_len)))\n",
    "        print('문장 길이 중간 값: {}'.format(np.median(text_len)))\n",
    "\n",
    "        # 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "        print('문장 길이 제 1 사분위: {}'.format(np.percentile(text_len, 25)))\n",
    "        print('문장 길이 제 3 사분위: {}'.format(np.percentile(text_len, 75)))\n",
    "            \n",
    "    def showWordCloud(self, text):\n",
    "        \"\"\"\n",
    "        WordCloud\n",
    "        \n",
    "        - input\n",
    "        : text / str / data['text'] (벡터화 하는데 사용되는 문자열)\n",
    "        \"\"\"\n",
    "        # 한글 폰트 깨짐방지\n",
    "        for font in [\"/Library/Fonts/NanumGothic.ttf\", \"/Library/Fonts/NotoSansCJKkr-Light.otf\"]:\n",
    "            if os.path.isfile(font):\n",
    "                FONT_PATH = font\n",
    "                break\n",
    "        cloud = WordCloud(font_path=FONT_PATH).generate(\" \".join(text))\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document.py\n",
    "- [awesome-devblog : feeds](https://awesome-devblog.now.sh/api/korean/people/feeds)\n",
    "- [nero google drive: labeled data](https://drive.google.com/drive/u/0/folders/1Npfrh6XmeABJ8JJ6ApS1T88vVoqyDH7M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./document.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./document.py\n",
    "import os, re, csv, requests, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from tqdm import trange\n",
    "from bs4 import BeautifulSoup\n",
    "from util import downloadByURL\n",
    "\n",
    "class KEYS(Enum):\n",
    "    # -1 : 아직 라벨링 안함 (default)\n",
    "    # 0  : 개발과 관련없는 문서\n",
    "    # 1  : 개발과 관련있는 문서\n",
    "    LABEL = 'label'\n",
    "    \n",
    "    # TAGS + TITLE + DESC\n",
    "    TEXT = 'text'\n",
    "    \n",
    "    # DATA_URL 결과 파싱용 Keys(Beans)\n",
    "    ID = '_id'\n",
    "    TITLE = 'title'\n",
    "    DESC = 'description'\n",
    "    TAGS = 'tags'\n",
    "    LINK = 'link'\n",
    "    \n",
    "    def getDocKeys():\n",
    "        \"\"\"\n",
    "        awesome-devblog API 요청시 가져오려는 컬럼\n",
    "        \n",
    "        - return\n",
    "        : list / 컬럼명 리스트\n",
    "        \"\"\"\n",
    "        return [KEYS.ID.value, KEYS.TITLE.value, KEYS.DESC.value, KEYS.TAGS.value, KEYS.LINK.value]\n",
    "    \n",
    "    def getTitleBlackList():\n",
    "        \"\"\"\n",
    "        title 컬럼 기준 블랙리스트\n",
    "        \n",
    "        - return\n",
    "        : list / 블랙리스트\n",
    "        \"\"\"\n",
    "        return ['', 'about']\n",
    "    \n",
    "    def getTextKeys():\n",
    "        \"\"\"\n",
    "        text 컬럼에 사용되는 awesome-devblog 컬럼\n",
    "        \n",
    "        - return\n",
    "        : list / 컬럼명 리스트\n",
    "        \"\"\"\n",
    "        return [KEYS.TAGS.value, KEYS.TITLE.value, KEYS.DESC.value]\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    def __init__(self, update=False):\n",
    "        \n",
    "        # Constant\n",
    "        self.DATA_URL = 'https://awesome-devblog.now.sh/api/korean/people/feeds'\n",
    "        self.DOCUMENTS_PATH = './data/documents.csv'\n",
    "        self.DOCUMENTS_URL = 'https://drive.google.com/uc?id=1K5Isidyb1O7OXQ47Yk2fMVYBvEoL6W4-&export=download'\n",
    "        self.MAX_REQ_SIZE = 5000\n",
    "        \n",
    "        # 기본 폴더 생성\n",
    "        for path in ['./data', './model', './wv_model']:\n",
    "            if not os.path.isdir(path):\n",
    "                os.makedirs(path)\n",
    "                \n",
    "        # ./data/documents.csv가 없는 경우 Google Driver에서 받아옴\n",
    "        # 자동 다운로드가 안될 경우 아래 경로에서 직접 받아 ./data 폴더 하위에 추가하면 됨\n",
    "        # https://drive.google.com/drive/u/0/folders/1Npfrh6XmeABJ8JJ6ApS1T88vVoqyDH7M\n",
    "        if not os.path.isfile(self.DOCUMENTS_PATH):\n",
    "            print('라벨링된 데이터를 다운로드합니다.')\n",
    "            downloadByURL(self.DOCUMENTS_URL, self.DOCUMENTS_PATH)\n",
    "        \n",
    "        if update:\n",
    "            self.updateDocs()\n",
    "        \n",
    "    def _getTotal(self):\n",
    "        \"\"\"\n",
    "        awesome-devblog에 전체 문서 개수 요청\n",
    "        \n",
    "        - return\n",
    "        : int / 전체 문서 개수\n",
    "        \"\"\"\n",
    "        res = requests.get(self.DATA_URL, { 'size': 1 })\n",
    "        res.raise_for_status()\n",
    "        doc = res.json()\n",
    "        return doc['total'][0]['count']\n",
    "\n",
    "    def _reqDoc(self, page, size, preprocessing=False):\n",
    "        \"\"\"\n",
    "        awesome-devblog에 문서 요청\n",
    "        : KEYS에 지정된 컬럼만 가져옴\n",
    "        \n",
    "        - input\n",
    "        : page / int / 요청 페이지(0부터 시작)\n",
    "        : size / int / 한 번의 요청으로 가져오려는 문서 개수\n",
    "        : preprocessing / boolean / 문서 전처리 여부\n",
    "        \n",
    "        - output\n",
    "        : DataFrame / DataFrame(response['data'])\n",
    "        \"\"\"\n",
    "        page += 1\n",
    "        params = {\n",
    "            'sort': 'date.asc',\n",
    "            'page': page,\n",
    "            'size': size\n",
    "        }\n",
    "        res = requests.get(self.DATA_URL, params)\n",
    "        res.raise_for_status()\n",
    "        doc = res.json()\n",
    "        \n",
    "        # json to dataframe\n",
    "        doc = pd.DataFrame(doc['data'], columns=KEYS.getDocKeys())\n",
    "        \n",
    "        # add label\n",
    "        doc.insert(0, KEYS.LABEL.value, -1)\n",
    "        \n",
    "        if preprocessing:\n",
    "            return self.preprocessing(doc)\n",
    "        else:\n",
    "            return doc\n",
    "\n",
    "    def _reqDocs(self, size, start_page=0):\n",
    "        \"\"\"\n",
    "        awesome-devblog에 전체 문서 요청\n",
    "        - input\n",
    "        : size / int / 한 번의 요청으로 가져올 문서개수(max 5000)\n",
    "        : start_page / int / 해당 페이지 부터 마지막 페이지까지 조회\n",
    "        \n",
    "        - return\n",
    "        : DataFrame / 전처리된 전체 데이터로 구성\n",
    "        \"\"\"\n",
    "        total = self._getTotal()\n",
    "        if size > self.MAX_REQ_SIZE: size = self.MAX_REQ_SIZE\n",
    "        total_req = round(total/size + 0.5)\n",
    "        docs = pd.DataFrame()\n",
    "        for i in trange(start_page, total_req):\n",
    "            doc = self._reqDoc(i, size)\n",
    "            if docs.empty:\n",
    "                docs = doc\n",
    "            else:\n",
    "                docs = docs.append(doc)\n",
    "        return self.preprocessing(docs)\n",
    "    \n",
    "    def preprocessing(self, doc, joinTags=True):\n",
    "        \"\"\"\n",
    "        문서 전처리\n",
    "        : tags / 배열로 되어있으므로 띄어쓰기로 join\n",
    "        : title, description, tags / 영어, 한글, 공백만 남김\n",
    "        : html tag 삭제\n",
    "        : \\n, \\r 삭제\n",
    "        : 2회 이상의 공백은 하나로 줄입\n",
    "        : 영어 대문자 소문자로 변환\n",
    "        : 앞뒤 공백 삭제\n",
    "        : 블랙리스트 데이터(KEYS.getTitleBlackList()) 제외\n",
    "        : text / tags + title + description 순서로 join된 컬럼 생성\n",
    "        \n",
    "        - input\n",
    "        : doc / DataFrame / documents.csv DataFrame\n",
    "        : joinTags / boolean / tags join 여부\n",
    "        \n",
    "        - return\n",
    "        : DataFrame / 전처리 완료된 데이터\n",
    "        \"\"\"\n",
    "        \n",
    "        # title, description, tags\n",
    "        def textPreprocessing(x):\n",
    "            x = BeautifulSoup(str(x), \"html.parser\").get_text()\n",
    "            x = re.sub('[^가-힣a-zA-Z\\s]', '', x)\n",
    "            return x\n",
    "        \n",
    "        # all\n",
    "        def docPreprocessing(x):\n",
    "            x = re.sub('[\\n\\r]', '', x)\n",
    "            x = re.sub('\\s{2,}', ' ', x)\n",
    "            x = x.lower()\n",
    "            x = x.strip()\n",
    "            return x\n",
    "        \n",
    "        for key in doc.columns:\n",
    "            if joinTags and KEYS(key) == KEYS.TAGS:\n",
    "                doc[key] = doc[key].apply(lambda x: ' '.join(x))\n",
    "            if key in KEYS.getTextKeys():\n",
    "                doc[key] = doc[key].apply(textPreprocessing)\n",
    "                \n",
    "            if key in KEYS.getDocKeys():\n",
    "                doc[key] = doc[key].apply(docPreprocessing)\n",
    "            \n",
    "        # remove blacklist\n",
    "        doc = doc.drop(doc[doc[KEYS.TITLE.value].isin(KEYS.getTitleBlackList())].index).reset_index()\n",
    "                        \n",
    "        # create text column\n",
    "        join_with = lambda x: ' '.join(x.dropna().astype(str))\n",
    "        doc[KEYS.TEXT.value] = doc[KEYS.getTextKeys()].apply(\n",
    "            join_with,\n",
    "            axis=1\n",
    "        )\n",
    "        return doc\n",
    "    \n",
    "    def getDocs(self, labeled_only=True):\n",
    "        \"\"\"\n",
    "        전체 문서 조회\n",
    "        - input\n",
    "        : labeled_only / boolean / 라벨링 된 데이터만 가져올지 선택\n",
    "        \n",
    "        - return\n",
    "        : DataFrame / documents.csv 데이터\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(self.DOCUMENTS_PATH):\n",
    "            print('> 문서가 없으므로 서버에 요청합니다.')\n",
    "            self.updateDocs()\n",
    "        data = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',', dtype={KEYS.LABEL.value: np.int64})\n",
    "        if not labeled_only:\n",
    "            return data\n",
    "        else:\n",
    "            return data.loc[data.label != -1]\n",
    "    \n",
    "    def updateDocs(self):\n",
    "        \"\"\"\n",
    "        awesome-devblog에 최신 문서 요청 및 documents.csv에 추가\n",
    "        : 데이터가 없는 경우, 전체 데이터를 가져옴\n",
    "        : 기존 데이터가 있는 경우, 없는 데이터만 추가\n",
    "       \n",
    "        - export\n",
    "        : ./data/documents.csv가 없는 경우 신규 생성\n",
    "        : ./data/documents.csv가 있는 경우 신규 문서 추가\n",
    "        \"\"\"\n",
    "        size = self.MAX_REQ_SIZE\n",
    "        \n",
    "        if not os.path.isfile(self.DOCUMENTS_PATH):\n",
    "            # 데이터가 없는 경우\n",
    "            docs = self._reqDocs(size)\n",
    "            docs.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "        else:\n",
    "            # 기존 데이터가 있는 경우\n",
    "            num_new_docs = 0\n",
    "            docs = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',')\n",
    "            total = self._getTotal()\n",
    "            total_docs = len(docs)\n",
    "            new_docs_num = total - total_docs\n",
    "            new_docs = self._reqDocs(size, total_docs // size)\n",
    "            \n",
    "            # _id가 기존 데이터에 존재하지 않는 경우에만 추가\n",
    "            docs = docs.append(new_docs[~new_docs[KEYS.ID.value].isin(docs[KEYS.ID.value])])\n",
    "            docs.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "            \n",
    "            if total_docs == len(docs):\n",
    "                print('> 문서가 최신 상태입니다.')\n",
    "            else:\n",
    "                print(f'> 신규 문서 {len(docs) - total_docs}개 추가')\n",
    "    \n",
    "    def syncDocLabel(self, old_document_path, sep, override=False):\n",
    "        \"\"\"\n",
    "        기존 라벨링한 데이터를 신규 문서에 반영\n",
    "        : title, link 기준으로 일치하는 문서 검색\n",
    "        \n",
    "        - input\n",
    "        : old_document_path / str / 기존 라벨링한 데이터 경로\n",
    "        : sep / str / csv delimiter\n",
    "        : override / boolean / 기존 라벨링이 반영된 결과를 ./data/documents.csv로 저장여부\n",
    "        \n",
    "        - export\n",
    "        : ./data/documents.csv\n",
    "        \"\"\"\n",
    "        \n",
    "        document = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',')\n",
    "        old_document = pd.read_csv(old_document_path, delimiter=sep)\n",
    "        self.preprocessing(old_document, joinTags=False)\n",
    "        for index, row in old_document.iterrows():\n",
    "            link = row.link\n",
    "            title = row.title\n",
    "            label = int(row.label)\n",
    "            if not len(document.loc[document.title.str.strip() == title.strip()]) and not len(document.loc[document.link == link]):\n",
    "                print(f'not found : {row.title}')\n",
    "            elif len(document.loc[document.title.str.strip() == title.strip()]):\n",
    "                document.loc[document.title.str.strip() == title.strip(), KEYS.LABEL.value] = label\n",
    "            elif len(document.loc[document.link == link]):\n",
    "                document.loc[document.link == link, KEYS.LABEL.value] = label\n",
    "        \n",
    "        # save synchronized document\n",
    "        if override:\n",
    "            document.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word_vector.py\n",
    "- [FastText wiki 한국어 데이터](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.bin.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./word_vector.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./word_vector.py\n",
    "import os\n",
    "import numpy as np\n",
    "from util import downloadByURL\n",
    "from gensim.models import FastText, fasttext # 둘이 다름 주의!\n",
    "\n",
    "\"\"\"\n",
    "FastText base word embedding\n",
    "\"\"\"\n",
    "class WordVector():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # corpus\n",
    "        self.WIKI_KO_DATA = './data/cc.ko.300.bin.gz'\n",
    "        self.WIKI_KO_DATA_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.bin.gz'\n",
    "\n",
    "        # pretrained model\n",
    "        self.WIKI_KO_MODEL_PATH = f'./wv_model/ko.wiki'\n",
    "\n",
    "    def getCustomModel(self, sentences, embedding_dim=4, window=3, min_count=1, epochs=10):\n",
    "        \"\"\"\n",
    "        주어진 문장들을 기반으로 FastText 단어 임베딩 모델 학습\n",
    "        \n",
    "        - input\n",
    "        : sentences / list / 학습에 사용될 문장 배열\n",
    "        : embedding_dim / int / 단어 벡터화시 차원 수\n",
    "        : window / int / 학습에 사용될 n-gram\n",
    "        : min_count / int / 학습에 사용될 단어의 최소 등장횟수\n",
    "        : epochs / int / 학습 횟수\n",
    "        \n",
    "        - return\n",
    "        : wv_model\n",
    "        \"\"\"\n",
    "        model = FastText(size=embedding_dim, window=window, min_count=min_count)\n",
    "        model.build_vocab(sentences=sentences)\n",
    "        model.train(sentences=sentences, total_examples=len(sentences), epochs=epochs)\n",
    "        return model\n",
    "    \n",
    "    def getWikiModel(self):\n",
    "        \"\"\"\n",
    "        위키 한국어 데이터를 기반으로 FastText 단어 임베딩 모델 학습\n",
    "        : 기존 학습된 모델이 있는 경우 해당 모델 반환\n",
    "        : 위키 한국어 데이터(./data/cc.ko.300.bin.gz)가 없는 경우 다운로드\n",
    "        : 기존 학습된 모델이 없는 경우 학습\n",
    "        : 학습된 결과를 ./wv_model에 저장\n",
    "        \n",
    "        - export\n",
    "        : self.WIKI_KO_MODEL_PATH\n",
    "        \"\"\"\n",
    "        model = None\n",
    "        if not os.path.isfile(self.WIKI_KO_MODEL_PATH):\n",
    "            print('학습된 단어 임베딩 모델이 없습니다.')\n",
    "            \n",
    "            if not os.path.isfile(self.WIKI_KO_DATA):\n",
    "                print('단어 임베딩 모델 학습에 필요한 데이터를 다운로드를 시작합니다.')\n",
    "                downloadByURL(self.WIKI_KO_DATA_URL, self.WIKI_KO_DATA)\n",
    "            \n",
    "            print('단어 임베딩 모델 학습을 시작합니다.')\n",
    "            model = fasttext.load_facebook_model(self.WIKI_KO_DATA)\n",
    "            \n",
    "            print('단어 임베딩 모델을 저장합니다.')\n",
    "            model.save(self.WIKI_KO_MODEL_PATH)\n",
    "        else:\n",
    "            model = FastText.load(self.WIKI_KO_MODEL_PATH)\n",
    "        \n",
    "        # print(f'vocab size : {len(model.wv.vocab)}') # 2,000,000\n",
    "        return model\n",
    "    \n",
    "    def getSimilarWords(self, wv_model, word, topn=5):\n",
    "        \"\"\"\n",
    "        유사단어 조회\n",
    "        \n",
    "        - input\n",
    "        : wv_model / FastText 단어 임베딩 모델\n",
    "        : word / str / 유사도를 측정하려는 단어\n",
    "        : topn / int / 조회 개수\n",
    "        \"\"\"\n",
    "        return wv_model.wv.similar_by_word(word, topn)\n",
    "    \n",
    "    def vectorization(self, wv_model, text, embedding_dim=300):\n",
    "        \"\"\"\n",
    "        주어진 문장을 단어별로 벡터화한 뒤 평균값을 문장의 벡터로 반환\n",
    "        \n",
    "        - input\n",
    "        : wv_model / FastText 단어 임베딩 모델\n",
    "        : text / str / 벡터화하려는 문장\n",
    "        : embedding_dim / int / wv_model vector의 차원 수 (wiki 기반 fasttext는 300차원)\n",
    "        \n",
    "        - return\n",
    "        : nparray / shape = (embedding_dim)\n",
    "        \"\"\"\n",
    "        words = text.split(' ')\n",
    "        words_num = len(words)\n",
    "        \n",
    "        # model dimension (wiki festtext의 경우 300)\n",
    "        vector = np.zeros(embedding_dim)\n",
    "        for word in words:\n",
    "            vector += wv_model[word]\n",
    "        return vector/words_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./classifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./classifier.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class Classifier():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 MODEL_PATH='./model/classifier.json',\n",
    "                 WEIGHT_PATH = './model/classifier.h5'):\n",
    "        self.MODEL_PATH = MODEL_PATH\n",
    "        self.WEIGHT_PATH = WEIGHT_PATH\n",
    "        self.history = None\n",
    "        \n",
    "    def _reshape(self, x):\n",
    "        \"\"\"\n",
    "        LSTM 계열의 레이어 사용시 필요한 (total, embedding_dim, 1) 형태의 shape로 변환\n",
    "        \n",
    "        - input\n",
    "        : x / nparray / 변환하려는 배열\n",
    "        \n",
    "        - return\n",
    "        : nparray\n",
    "        \"\"\"\n",
    "        return x.reshape(x.shape[0], x.shape[1], 1)\n",
    "    \n",
    "    def _dataSeperator(self, data, test_size=0.33):\n",
    "        \"\"\"\n",
    "        데이터 분할\n",
    "        \n",
    "        - input\n",
    "        : data / DataFrame / documents.csv 데이터\n",
    "        : test_size / float / 데이터 분할 비율\n",
    "        \n",
    "        - return\n",
    "        : [nparray, nparray, nparray, nparray]\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.vector,\n",
    "                                                            data.label,\n",
    "                                                            test_size=test_size,\n",
    "                                                            random_state=321)\n",
    "        X_train = np.array(X_train.tolist(), dtype=np.float32)\n",
    "        X_test = np.array(X_test.tolist(), dtype=np.float32)\n",
    "        y_train = np.array(y_train.tolist(), dtype=np.int32)\n",
    "        y_test = np.array(y_test.tolist(), dtype=np.int32)\n",
    "        return X_train, X_test, np.asarray(y_train), np.asarray(y_test)\n",
    "        \n",
    "    def train(self,\n",
    "              data,\n",
    "              checkpoint_path,\n",
    "              epochs=75,\n",
    "              batch_size=100,\n",
    "              validation_split=0.1,\n",
    "              verbose=0):\n",
    "        \"\"\"\n",
    "        모델 학습\n",
    "    \n",
    "        - input\n",
    "        : data / DataFrame / documents.csv 데이터\n",
    "        : checkpoint_path / str / 학습 중간 결과물 저장 경로\n",
    "        : epochs / int / 학습 횟수\n",
    "        : batch_size / int / 배치 사이즈\n",
    "        : validation_split / float / validation data ratio\n",
    "        : verbose / int / 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "        - return\n",
    "        : classifier\n",
    "        \n",
    "        - export\n",
    "        : ./model/classifier.json (graph)\n",
    "        : ./model/classifier.h5 (weights)\n",
    "        \"\"\"\n",
    "        \n",
    "        # seperate data\n",
    "        X_train, X_test, y_train, y_test = self._dataSeperator(data)\n",
    "        \n",
    "        # model\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))\n",
    "        model.add(Dense(80, activation='relu', kernel_initializer='he_normal'))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['acc',\n",
    "                               self.f1_m,\n",
    "                               self.precision_m,\n",
    "                               self.recall_m])\n",
    "        model.summary()\n",
    "        \n",
    "        # checkpoint\n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=2, save_best_only=True)\n",
    "        callbacks_list = [checkpoint]\n",
    "        \n",
    "        self.history = model.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=validation_split,\n",
    "                            callbacks=callbacks_list)\n",
    "        loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=verbose)\n",
    "        print(f'loss : {loss}')\n",
    "        print(f'accuracy : {accuracy}')\n",
    "        print(f'f1_score : {f1_score}')\n",
    "        print(f'precision : {precision}')\n",
    "        print(f'recall : {recall}')\n",
    "        self.saveModel(model)\n",
    "        return model\n",
    "        \n",
    "    def saveModel(self, model):\n",
    "        \"\"\"\n",
    "        모델의 parameter와 weights를 저장한다.\n",
    "        \n",
    "        - input\n",
    "        : model / classifier\n",
    "        \n",
    "        - export\n",
    "        : ./model/classifier.json / parameter\n",
    "        : ./model/classifier.h5 / weights\n",
    "        \"\"\"\n",
    "        # save model\n",
    "        model_json = model.to_json()\n",
    "        with open(self.MODEL_PATH, \"w\") as json_file : \n",
    "            json_file.write(model_json)\n",
    "        \n",
    "        # save weights\n",
    "        model.save_weights(self.WEIGHT_PATH)\n",
    "        \n",
    "    def loadModel(self):\n",
    "        \"\"\"\n",
    "        모델을 불러옴\n",
    "        \n",
    "        - return\n",
    "        : classifier\n",
    "        \"\"\"\n",
    "        # load model\n",
    "        with open(self.MODEL_PATH, \"r\") as json_file:\n",
    "            json_model = json_file.read()\n",
    "        model = model_from_json(json_model)\n",
    "        \n",
    "        # load weight\n",
    "        model.load_weights(self.WEIGHT_PATH)\n",
    "        return model\n",
    "        \n",
    "    def showHistory(self):\n",
    "        \"\"\"\n",
    "        train history를 그래프로 나타냄\n",
    "        \"\"\"\n",
    "        if self.history == None:\n",
    "            print('학습내역이 없습니다.')\n",
    "            return\n",
    "        \n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "        acc_ax.plot(self.history.history['acc'], 'b', label='train acc')\n",
    "        acc_ax.plot(self.history.history['val_acc'], 'g', label='val acc')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "        acc_ax.legend(loc='upper left')\n",
    "        plt.show()\n",
    "    \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        재현율(실제 True인 것 중에서 모델이 True라고 예측한 것의 비율) 계산\n",
    "        \n",
    "        - input\n",
    "        : y_true / int / 정답\n",
    "        : y_pred / int / 모델 예측결과\n",
    "        \n",
    "        - return\n",
    "        : float\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        정밀도(모델이 True라고 분류한 것 중에서 실제 True인 것의 비율) 계산\n",
    "        \n",
    "        - input\n",
    "        : y_true / int / 정답\n",
    "        : y_pred / int / 모델 예측결과\n",
    "        \n",
    "        - return\n",
    "        : float\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        F1 score(Precision과 Recall의 조화평균) 계산\n",
    "        \n",
    "        - input\n",
    "        : y_true / int / 정답\n",
    "        : y_pred / int / 모델 예측결과\n",
    "        \n",
    "        - return\n",
    "        : float\n",
    "        \"\"\"\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2 * ((precision * recall)/(precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc",
   "language": "python",
   "name": "dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
