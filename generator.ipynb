{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./util.py\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def downloadByURL(url, output_path):\n",
    "    \"\"\"\n",
    "    HTTP 파일 다운로드\n",
    "    \"\"\"\n",
    "    class DownloadProgressBar(tqdm):\n",
    "        def update_to(self, b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                self.total = tsize\n",
    "            self.update(b * bsize - self.n)\n",
    "        \n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "        \n",
    "def oneHotEncoding(label, classNum):\n",
    "    \"\"\"\n",
    "    라벨링된 int를 oneHot 인코딩한다\n",
    "    ex) oneHotEncoding(0, 2) -> [1, 0]\n",
    "    ex) oneHotEncoding(1, 2) -> [0, 1]\n",
    "    \"\"\"\n",
    "    oneHot = [0]*classNum\n",
    "    oneHot[label] = 1\n",
    "    return oneHot\n",
    "\n",
    "def reshape(series, embedding_dim):\n",
    "    \"\"\"\n",
    "    shape 변경\n",
    "    \"\"\"\n",
    "    result = np.array(series.tolist())\n",
    "    result = result.reshape(result.shape[0], embedding_dim, 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./analysis.py\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class Analysis():\n",
    "     \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        전체 분석\n",
    "        \"\"\"\n",
    "        self.countAnalysis(data)\n",
    "        print()\n",
    "        self.textAnalysis(data)\n",
    "        print()\n",
    "        self.showWordCloud(data.text)\n",
    "        \n",
    "    def countAnalysis(self, data):\n",
    "        \"\"\"\n",
    "        데이터 수량 조사\n",
    "        \"\"\"\n",
    "        \n",
    "        labeled_data = data.loc[data.label != -1]\n",
    "        total_count = len(data) # 전체 데이터 수\n",
    "        labeled_count = len(labeled_data) # 라벨링 된 데이터 수\n",
    "\n",
    "        print('> 데이터 수량 조사')\n",
    "        print(f'전체 데이터 수: {total_count}개')\n",
    "        print(f'라벨링된 데이터 수: {labeled_count}개')\n",
    "        for label, count in data.label.value_counts().iteritems():\n",
    "            print(f'class {label} : {count}개')\n",
    "    \n",
    "    def textAnalysis(self, data):\n",
    "        \"\"\"\n",
    "        text 길이 분석\n",
    "        \"\"\"\n",
    "        text_len = data.text.apply(len)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.hist(text_len, bins=200, alpha=0.5, color= 'r', label='length of text')\n",
    "        plt.legend(fontsize='x-large')\n",
    "        plt.yscale('log', nonposy='clip')\n",
    "        plt.title('Log-Histogram of length of text')\n",
    "        plt.xlabel('Length of text')\n",
    "        plt.ylabel('Number of text')\n",
    "\n",
    "        print('> 문장 길이 분석')\n",
    "        print('문장 길이 최대 값: {}'.format(np.max(text_len)))\n",
    "        print('문장 길이 최소 값: {}'.format(np.min(text_len)))\n",
    "        print('문장 길이 평균 값: {:.2f}'.format(np.mean(text_len)))\n",
    "        print('문장 길이 표준편차: {:.2f}'.format(np.std(text_len)))\n",
    "        print('문장 길이 중간 값: {}'.format(np.median(text_len)))\n",
    "\n",
    "        # 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "        print('문장 길이 제 1 사분위: {}'.format(np.percentile(text_len, 25)))\n",
    "        print('문장 길이 제 3 사분위: {}'.format(np.percentile(text_len, 75)))\n",
    "            \n",
    "    def showWordCloud(self, text):\n",
    "        \"\"\"\n",
    "        WordCloud\n",
    "        \"\"\"\n",
    "        # 한글 폰트 깨짐방지\n",
    "        for font in [\"/Library/Fonts/NanumGothic.ttf\", \"/Library/Fonts/NotoSansCJKkr-Light.otf\"]:\n",
    "            if os.path.isfile(font):\n",
    "                FONT_PATH = font\n",
    "                break\n",
    "        cloud = WordCloud(font_path=FONT_PATH).generate(\" \".join(text))\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document.py\n",
    "- [awesome-devblog : feeds](https://awesome-devblog.now.sh/api/korean/people/feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./document.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./document.py\n",
    "import os, re, csv, requests, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "from tqdm import trange\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class KEYS(Enum):\n",
    "    # -1 : 아직 라벨링 안함 (default)\n",
    "    # 0  : 개발과 관련없는 문서\n",
    "    # 1  : 개발과 관련있는 문서\n",
    "    LABEL = 'label'\n",
    "    \n",
    "    # TAGS + TITLE + DESC\n",
    "    TEXT = 'text'\n",
    "    \n",
    "    # DATA_URL 결과 파싱용 Keys(Beans)\n",
    "    ID = '_id'\n",
    "    TITLE = 'title'\n",
    "    DESC = 'description'\n",
    "    TAGS = 'tags'\n",
    "    LINK = 'link'\n",
    "    \n",
    "    def getDocKeys():\n",
    "        return [KEYS.ID.value, KEYS.TITLE.value, KEYS.DESC.value, KEYS.TAGS.value, KEYS.LINK.value]\n",
    "    \n",
    "    def getTitleBlackList():\n",
    "        return ['', 'about']\n",
    "    \n",
    "    def getTextKeys():\n",
    "        return [KEYS.TAGS.value, KEYS.TITLE.value, KEYS.DESC.value]\n",
    "\n",
    "class Document():\n",
    "    \n",
    "    def __init__(self, update=False):\n",
    "        \n",
    "        # Constant\n",
    "        self.DATA_URL = 'https://awesome-devblog.now.sh/api/korean/people/feeds'\n",
    "        self.DOCUMENTS_PATH = './data/documents.csv'\n",
    "        self.MAX_REQ_SIZE = 5000\n",
    "        \n",
    "        # 기본 폴더 생성\n",
    "        for path in ['./data', './model', './wv_model']:\n",
    "            if not os.path.isdir(path):\n",
    "                os.makedirs(path)\n",
    "        \n",
    "        if update:\n",
    "            self.updateDocs()\n",
    "        \n",
    "    def _getTotal(self):\n",
    "        \"\"\"\n",
    "        전체 문서 개수 요청\n",
    "        \"\"\"\n",
    "        res = requests.get(self.DATA_URL, { 'size': 1 })\n",
    "        res.raise_for_status()\n",
    "        doc = res.json()\n",
    "        return doc['total'][0]['count']\n",
    "\n",
    "    def _reqDoc(self, page, size, preprocessing=False):\n",
    "        \"\"\"\n",
    "        문서 요청\n",
    "        - page는 0 부터 시작\n",
    "        - 전처리(self._preprocessing) 후 반환\n",
    "        \"\"\"\n",
    "        page += 1\n",
    "        params = {\n",
    "            'sort': 'date.asc',\n",
    "            'page': page,\n",
    "            'size': size\n",
    "        }\n",
    "        res = requests.get(self.DATA_URL, params)\n",
    "        res.raise_for_status()\n",
    "        doc = res.json()\n",
    "        \n",
    "        # json to dataframe\n",
    "        doc = pd.DataFrame(doc['data'], columns=KEYS.getDocKeys())\n",
    "        \n",
    "        # add label\n",
    "        doc.insert(0, KEYS.LABEL.value, -1)\n",
    "        \n",
    "        if preprocessing:\n",
    "            return self._preprocessing(doc)\n",
    "        else:\n",
    "            return doc\n",
    "    \n",
    "    def _preprocessing(self, doc, joinTags=True):\n",
    "        \"\"\"\n",
    "        문서 전처리\n",
    "        - KEYS 이외의 key 삭제\n",
    "        - [tag] list join to string\n",
    "        - [title / description / tags] 영어, 한글, 공백 이외의 것들 모두 삭제\n",
    "        - html tag 삭제\n",
    "        - \\n, \\r 삭제\n",
    "        - 2번 이상의 공백 1개로 통합\n",
    "        - 영어 대문자 소문자로 변환\n",
    "        - 앞뒤 공백 삭제\n",
    "        - text 컬럼 생성 : text = tags + title + description\n",
    "        \"\"\"\n",
    "        \n",
    "        # title, description, tags\n",
    "        def textPreprocessing(x):\n",
    "            x = BeautifulSoup(str(x), \"html.parser\").get_text()\n",
    "            x = re.sub('[^가-힣a-zA-Z\\s]', '', x)\n",
    "            return x\n",
    "        \n",
    "        # all\n",
    "        def docPreprocessing(x):\n",
    "            x = re.sub('[\\n\\r]', '', x)\n",
    "            x = re.sub('\\s{2,}', ' ', x)\n",
    "            x = x.lower()\n",
    "            x = x.strip()\n",
    "            return x\n",
    "        \n",
    "        for key in doc.columns:\n",
    "            if joinTags and KEYS(key) == KEYS.TAGS:\n",
    "                doc[key] = doc[key].apply(lambda x: ' '.join(x))\n",
    "            if key in KEYS.getTextKeys():\n",
    "                doc[key] = doc[key].apply(textPreprocessing)\n",
    "                \n",
    "            if key in KEYS.getDocKeys():\n",
    "                doc[key] = doc[key].apply(docPreprocessing)\n",
    "            \n",
    "        # remove blacklist\n",
    "        doc = doc.drop(doc[doc[KEYS.TITLE.value].isin(KEYS.getTitleBlackList())].index).reset_index()\n",
    "                        \n",
    "        # create text column\n",
    "        join_with = lambda x: ' '.join(x.dropna().astype(str))\n",
    "        doc[KEYS.TEXT.value] = doc[KEYS.getTextKeys()].apply(\n",
    "            join_with,\n",
    "            axis=1\n",
    "        )\n",
    "        return doc\n",
    "        \n",
    "\n",
    "    def _reqDocs(self, size, start_page=0):\n",
    "        \"\"\"\n",
    "        전체 문서 요청\n",
    "        \"\"\"\n",
    "        total = self._getTotal()\n",
    "        if size > self.MAX_REQ_SIZE: size = self.MAX_REQ_SIZE\n",
    "        total_req = round(total/size + 0.5)\n",
    "        docs = pd.DataFrame()\n",
    "        for i in trange(start_page, total_req):\n",
    "            doc = self._reqDoc(i, size)\n",
    "            if docs.empty:\n",
    "                docs = doc\n",
    "            else:\n",
    "                docs = docs.append(doc)\n",
    "        return self._preprocessing(docs)\n",
    "    \n",
    "    def getDocs(self, labeled_only=True):\n",
    "        \"\"\"\n",
    "        전체 문서 조회\n",
    "        labeled\n",
    "        :True = 라벨링 된 데이터만 가져오기\n",
    "        :False = 전체 데이터 가져오기\n",
    "        \"\"\"\n",
    "        if not os.path.isfile(self.DOCUMENTS_PATH):\n",
    "            print('> 문서가 없으므로 서버에 요청합니다.')\n",
    "            self.updateDocs()\n",
    "        data = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',', dtype={KEYS.LABEL.value: np.int64})\n",
    "        if not labeled_only:\n",
    "            return data\n",
    "        else:\n",
    "            return data.loc[data.label != -1]\n",
    "    \n",
    "    def updateDocs(self):\n",
    "        \"\"\"\n",
    "        최신 문서 추가\n",
    "        - 데이터가 없는 경우, 전체 데이터를 가져옴\n",
    "        - 기존 데이터가 있는 경우, 없는 데이터만 추가\n",
    "        \"\"\"\n",
    "        size = self.MAX_REQ_SIZE\n",
    "        \n",
    "        if not os.path.isfile(self.DOCUMENTS_PATH):\n",
    "            # 데이터가 없는 경우\n",
    "            docs = self._reqDocs(size)\n",
    "            docs.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "        else:\n",
    "            # 기존 데이터가 있는 경우\n",
    "            num_new_docs = 0\n",
    "            docs = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',')\n",
    "            total = self._getTotal()\n",
    "            total_docs = len(docs)\n",
    "            new_docs_num = total - total_docs\n",
    "            new_docs = self._reqDocs(size, total_docs // size)\n",
    "            \n",
    "            # _id가 기존 데이터에 존재하지 않는 경우에만 추가\n",
    "            docs = docs.append(new_docs[~new_docs[KEYS.ID.value].isin(docs[KEYS.ID.value])])\n",
    "            docs.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "            \n",
    "            if total_docs == len(docs):\n",
    "                print('> 문서가 최신 상태입니다.')\n",
    "            else:\n",
    "                print(f'> 신규 문서 {len(docs) - total_docs}개 추가')\n",
    "    \n",
    "    def syncDocLabel(self, old_document_path, sep, override=False):\n",
    "        \"\"\"\n",
    "        기존 라벨링한 데이터를 신규 문서에 반영\n",
    "        - title, link 기준으로 일치하는 문서 검색\n",
    "        \"\"\"\n",
    "        \n",
    "        document = pd.read_csv(self.DOCUMENTS_PATH, delimiter=',')\n",
    "        old_document = pd.read_csv(old_document_path, delimiter=sep)\n",
    "        self._preprocessing(old_document, joinTags=False)\n",
    "        for index, row in old_document.iterrows():\n",
    "            link = row.link\n",
    "            title = row.title\n",
    "            label = int(row.label)\n",
    "            if not len(document.loc[document.title.str.strip() == title.strip()]) and not len(document.loc[document.link == link]):\n",
    "                print(f'not found : {row.title}')\n",
    "            elif len(document.loc[document.title.str.strip() == title.strip()]):\n",
    "                document.loc[document.title.str.strip() == title.strip(), KEYS.LABEL.value] = label\n",
    "            elif len(document.loc[document.link == link]):\n",
    "                document.loc[document.link == link, KEYS.LABEL.value] = label\n",
    "        \n",
    "        # save synchronized document\n",
    "        if override:\n",
    "            document.to_csv(self.DOCUMENTS_PATH, sep=\",\", index=False)\n",
    "        print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word_vector.py\n",
    "- [FastText wiki 한국어 데이터](https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.bin.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./word_vector.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./word_vector.py\n",
    "import os\n",
    "import numpy as np\n",
    "from util import downloadByURL\n",
    "from gensim.models import FastText, fasttext # 둘이 다름 주의!\n",
    "\n",
    "\"\"\"\n",
    "FastText base word embedding\n",
    "\"\"\"\n",
    "class WordVector():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # corpus\n",
    "        self.WIKI_KO_DATA = './data/cc.ko.300.bin.gz'\n",
    "        self.WIKI_KO_DATA_URL = 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ko.300.bin.gz'\n",
    "\n",
    "        # pretrained model\n",
    "        self.WIKI_KO_MODEL_PATH = f'./wv_model/ko.wiki'\n",
    "\n",
    "    def getCustomModel(self, text, size=4, window=3, min_count=1, epochs=10):\n",
    "        \"\"\"\n",
    "        FastText 기반 모델 학습\n",
    "        \"\"\"\n",
    "        model = FastText(size=size, window=window, min_count=min_count)\n",
    "        model.build_vocab(sentences=text)\n",
    "        model.train(sentences=text, total_examples=len(text), epochs=epochs)\n",
    "        return model\n",
    "    \n",
    "    def getWikiModel(self):\n",
    "        \"\"\"\n",
    "        위키 한국어 데이터 기반 모델 학습\n",
    "        \"\"\"\n",
    "        model = None\n",
    "        if not os.path.isfile(self.WIKI_KO_MODEL_PATH):\n",
    "            print('학습된 모델이 없습니다.')\n",
    "            \n",
    "            if not os.path.isfile(self.WIKI_KO_DATA):\n",
    "                print('모델 학습에 필요한 데이터를 다운로드를 시작합니다.')\n",
    "                downloadByURL(self.WIKI_KO_DATA_URL, self.WIKI_KO_DATA)\n",
    "            \n",
    "            print('모델 학습을 시작합니다.')\n",
    "            model = fasttext.load_facebook_model(self.WIKI_KO_DATA)\n",
    "            model.save(self.WIKI_KO_MODEL_PATH)\n",
    "            \n",
    "        else:\n",
    "            model = FastText.load(self.WIKI_KO_MODEL_PATH)\n",
    "        \n",
    "        print(f'vocab size : {len(model.wv.vocab)}')\n",
    "        return model\n",
    "    \n",
    "    def getSimilarWords(self, wv_model, word, topn=5):\n",
    "        \"\"\"\n",
    "        유사단어 조회\n",
    "        \"\"\"\n",
    "        return wv_model.wv.similar_by_word(word, topn)\n",
    "    \n",
    "    def vectorization(self, wv_model, text, embedding_dim=300):\n",
    "        \"\"\"\n",
    "        주어진 문장을 단어별로 벡터화한 뒤 평균값을 문장의 벡터로 반환\n",
    "        embedding_dim : wv_model vector의 차원 수 (wiki 기반 fasttext는 300차원)\n",
    "        \"\"\"\n",
    "        words = text.split(' ')\n",
    "        words_num = len(words)\n",
    "        \n",
    "        # model dimension (wiki festtext의 경우 300)\n",
    "        vector = np.zeros(embedding_dim)\n",
    "        for word in words:\n",
    "            vector += wv_model[word]\n",
    "        return vector/words_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./main.py\n",
    "from util import oneHotEncoding\n",
    "from document import Document\n",
    "from analysis import Analysis\n",
    "from word_vector import WordVector\n",
    "\n",
    "def ready_data():\n",
    "    doc = Document()\n",
    "    wv = WordVector()\n",
    "    wv_model = wv.getWikiModel()\n",
    "    \n",
    "    embedding_dim = 300\n",
    "    \n",
    "    # 라벨링 된 데이터만 가져오기\n",
    "    data = doc.getDocs(True) \n",
    "    \n",
    "    # 전체 데이터에 대해서 분석\n",
    "    # Analysis(doc.getDocs(labeled_only=False))\n",
    "    \n",
    "    # 임베딩 테스트\n",
    "    # wv.getSimilarWords(wv_model, '파이썬', 5)\n",
    "    \n",
    "    # one hot encoding\n",
    "    data.label = data.label.apply(lambda x: oneHotEncoding(x, 2))\n",
    "    \n",
    "    # vectorization\n",
    "    data['vector'] = data.text.apply(lambda x: wv.vectorization(wv_model, x, embedding_dim))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 2000000\n"
     ]
    }
   ],
   "source": [
    "data = ready_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./classifier.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from util import reshape\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "\n",
    "class Classifier():\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def _dataSeperator(self, data, embedding_dim):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.vector,\n",
    "                                                            data.label,\n",
    "                                                            test_size=0.33,\n",
    "                                                            random_state=321)\n",
    "        X_train = reshape(X_train, embedding_dim)\n",
    "        X_test = reshape(X_test, embedding_dim)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def train(self,\n",
    "              data,\n",
    "              embedding_dim=300,\n",
    "              epochs=75,\n",
    "              batch_size=100,\n",
    "              validation_split=0.3,\n",
    "              verbose=0):\n",
    "        # seperate data\n",
    "        X_train, X_test, y_train, y_test = self._dataSeperator(data, embedding_dim)\n",
    "        print(type(X_train), type(X_test), type(y_train), type(y_test))\n",
    "        \n",
    "        # layer\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Dense(embedding_dim, input_shape=(X_train.shape[1], 1), activation='relu'))\n",
    "        model.add(SimpleRNN(32))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['acc',\n",
    "                               self.f1_m,\n",
    "                               self.precision_m,\n",
    "                               self.recall_m])\n",
    "\n",
    "        history = model.fit(X_train,\n",
    "                            np.asarray(y_train),\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_split=validation_split)\n",
    "        \n",
    "        loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, np.asarray(y_test), verbose=verbose)\n",
    "        print(f'loss : {loss}')\n",
    "        print(f'accuracy : {accuracy}')\n",
    "        print(f'f1_score : {f1_score}')\n",
    "        print(f'precision : {precision}')\n",
    "        print(f' recall : {recall}')\n",
    "        \n",
    "    def showHistory(self, history):\n",
    "        fig, loss_ax = plt.subplots()\n",
    "        acc_ax = loss_ax.twinx()\n",
    "\n",
    "        acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "        acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "        acc_ax.set_ylabel('accuracy')\n",
    "        acc_ax.legend(loc='upper left')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self):\n",
    "        return\n",
    "    \n",
    "    def recall_m(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(self, y_true, y_pred):\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            return precision\n",
    "\n",
    "    def f1_m(self, y_true, y_pred):\n",
    "        precision = self.precision_m(y_true, y_pred)\n",
    "        recall = self.recall_m(y_true, y_pred)\n",
    "        return 2 * ((precision * recall)/(precision + recall + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'pandas.core.series.Series'> <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6fbe79d9a835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m          verbose=0)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-66ff22644a1d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, embedding_dim, epochs, batch_size, validation_split, verbose)\u001b[0m\n\u001b[1;32m     51\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                             validation_split=0.3)\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         distribution_strategy=distribution_strategy)\n\u001b[0m\u001b[1;32m    529\u001b[0m     val_adapter = adapter_cls(val_x, val_y,\n\u001b[1;32m    530\u001b[0m                               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[1;32m    320\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     ))\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    412\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2333\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2336\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 111\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1182\u001b[0m   preferred_dtype = deprecation.deprecated_argument_lookup(\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1240\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \"\"\"\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dc/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "cf.train(data,\n",
    "         embedding_dim=300,\n",
    "         epochs=2,\n",
    "         batch_size=100,\n",
    "         validation_split=0.3,\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6955, 300, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dc",
   "language": "python",
   "name": "dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
